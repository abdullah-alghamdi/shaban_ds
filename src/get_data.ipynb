{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will parse the Medina Amana website for death records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import os\n",
    "import sys\n",
    "import ummalqura\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medina_url = \"https://services.amana-md.gov.sa/eservicesite/Inq/DeathInquiry.aspx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to post request the commands to the websites, but it seems there is an issue with the protocols. Instead, I will use Chrome webdriver and \"find_element\" in the DOM to access and scrape the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the browser\n",
    "browser = webdriver.Chrome(\"./chromedriver\")\n",
    "browser.get(medina_url)\n",
    "browser.implicitly_wait(5)\n",
    "\n",
    "#start parsing from 1340/1/1 \n",
    "m_from_day = Select(browser.find_element_by_id('ctl00_ContentPlaceHolder1_cboDFrom')).select_by_value('01')\n",
    "m_from_month = Select(browser.find_element_by_id('ctl00_ContentPlaceHolder1_cboMFrom')).select_by_value('01')\n",
    "m_from_year = Select(browser.find_element_by_id('ctl00_ContentPlaceHolder1_cboYFrom')).select_by_value('1340')\n",
    "\n",
    "# to 1440/11/12\n",
    "m_to_day = Select(browser.find_element_by_id('ctl00_ContentPlaceHolder1_cboDTo')).select_by_value('12')\n",
    "m_to_month = Select(browser.find_element_by_id('ctl00_ContentPlaceHolder1_cboMTo')).select_by_value('11')\n",
    "m_to_year = Select(browser.find_element_by_id('ctl00_ContentPlaceHolder1_cboYTo')).select_by_value('1440')\n",
    "\n",
    "# create a list to store pages \n",
    "page_store = []\n",
    "\n",
    "browser.find_element_by_id('ctl00_ContentPlaceHolder1_btnSubmit').click()\n",
    "\n",
    "# make sure to wait for the div to be downloaded\n",
    "element = WebDriverWait(browser, 10).until(lambda x: x.find_element_by_id('ctl00_ContentPlaceHolder1_dgDeath'))\n",
    "# and store\n",
    "page_store.append(bs(browser.page_source, 'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's automate the navigation \n",
    "# IMPORTANT NOTE: this automation might stop at some point due to some changes in the dynamics of the page as you click. Therefore, when it stops try  \n",
    "# to do the process again from where you stopped by changing the date in the section above (change m_from_day, m_from_month and m_from_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the indices of the spans to click\n",
    "spans = [i for i in range(1,22)]  \n",
    "\n",
    "#span text holder to debug the process \n",
    "span_text = []\n",
    "#span index holder to debug the index\n",
    "span_index = []\n",
    "\n",
    "# you need to change the the span index after the second and the third iterations\n",
    "# this var is to control the change \n",
    "change_index = 0\n",
    "\n",
    "# always use try\n",
    "try:\n",
    "    while True:\n",
    "        \n",
    "        for span in spans:\n",
    "            span_index.append(span)\n",
    "            # find where to click using inspect\n",
    "            span_holder = browser.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_dgDeath\"]/tbody/tr[78]/td/a[%s]' % span)[0]\n",
    "            print(\"span text: \", span_holder.text)\n",
    "            print(\"span index: \", span)\n",
    "            span_text.append(span_holder.text)\n",
    "            #show us where we are\n",
    "            # click \n",
    "            span_holder.click()\n",
    "            # append to page store \n",
    "            page_store.append(bs(browser.page_source, 'html.parser'))\n",
    "            \n",
    "        if change_index ==1 :\n",
    "            print(\"change 1\")\n",
    "            # in the second iteration \n",
    "            # you need to delete the one and add another 21 to access the next set of spans \n",
    "            spans = [i for i in range(2,22)] + [21] \n",
    "        if change_index ==2:\n",
    "            print(\"change 2\")\n",
    "            # in the third iteration\n",
    "            # you need to delete the 21 you added in the second iteration\n",
    "            spans = [i for i in range(2,22)] \n",
    "        \n",
    "        change_index +=1\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Something wrong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data from pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first initialize placeholders: \n",
    "names = []\n",
    "sex = []\n",
    "nationality = []\n",
    "DOD = []\n",
    "age=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the process of extracting from the html layout\n",
    "\n",
    "for p in page_store:\n",
    "    div_ = p.find('table', {'id': 'ctl00_ContentPlaceHolder1_dgDeath'})\n",
    "    tr = div_.findAll('tr')\n",
    "    \n",
    "    # the data contained here\n",
    "    container = tr[2:-1]\n",
    "\n",
    "    for row in container:\n",
    "        try:\n",
    "            names.append(row.findAll('td')[0].text.strip())\n",
    "        # fill with np.nan if data doesn't exist\n",
    "        except:\n",
    "            names.append(np.nan)\n",
    "\n",
    "        try:\n",
    "            sex.append(row.findAll('td')[1].text.strip())\n",
    "\n",
    "        except:\n",
    "            sex.append(np.nan)\n",
    "\n",
    "        try:\n",
    "            nationality.append(row.findAll('td')[2].text.strip())\n",
    "        except:\n",
    "            nationality.append(np.nan)\n",
    "\n",
    "        try:\n",
    "            age.append(row.findAll('td')[3].text.strip())\n",
    "        except:\n",
    "            age.append(np.nan)\n",
    "\n",
    "        try:\n",
    "            DOD.append(row.findAll('td')[4].text.strip())\n",
    "        except:\n",
    "            DOD.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save to a dataFrame and then to csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name': names, 'sex': sex, 'nationality': nationality, 'age': age, 'DOD': DOD})\n",
    "df.to_csv('data_m.csv',  encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
